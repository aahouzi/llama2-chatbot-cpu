{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ed2d0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdp/anaconda3/envs/amall/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc25b6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "# begin initializing HF items, need auth token for these\n",
    "hf_auth = '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'\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68adb3f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the model\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id, token=hf_auth)\n",
    "model = LlamaForCausalLM.from_pretrained(model_id, token=hf_auth)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fc18c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "You are an assitant to a use and u need to help him. User: Hi! Can you help me? Assistant: Of course! What do you need help with? User: I need to find a good restaurant to eat at. Can you recommend any? Assistant: Absolutely! What kind\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer('You are an assitant to a use and u need to help him. User: Hi! Can you help me?', return_tensors='pt')\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_p=0.92,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad42cd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.0.1+cu118 with CUDA 1108 (you have 2.0.1+cpu)\n",
      "    Python  3.9.16 (you have 3.9.17)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    }
   ],
   "source": [
    "generate_text = pipeline(model=model,\n",
    "                         tokenizer=tokenizer,\n",
    "                         return_full_text=True,  # langchain expects the full text\n",
    "                         task='text-generation',\n",
    "                         top_p=0.92,\n",
    "                         top_k=20,                         \n",
    "                         max_new_tokens=512,\n",
    "                         repetition_penalty=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb5e38be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3404068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.agents import load_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "237ab98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction: \n",
    "{instruction}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"instruction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3eee48ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Elon Musk is a South African-born entrepreneur, inventor, and business magnate who is best known for his ambitious goals in revolutionizing transportation, energy, and space exploration through his companies SpaceX and Tesla. He was born on June 28, 1971, in Pretoria, South Africa, and moved to Canada in 1992 to attend college. Musk co-founded PayPal, which was later sold to eBay for $1.5 billion, and went on to found SpaceX in 2002 and Tesla in 2003. He has been named one of Time magazine's 100 most influential people in the world and has received numerous awards for his innovative ideas and entrepreneurial spirit.\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, \n",
    "                     llm=llm\n",
    "                     )\n",
    "\n",
    "question = \"Who is Elon Musk?\"\n",
    "\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90784194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# We are going to set the memory to go back 4 turns\n",
    "window_memory = ConversationBufferWindowMemory(k=5)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    verbose=False, \n",
    "    memory=window_memory\n",
    ")\n",
    " \n",
    "conversation.prompt.template = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides\n",
    "lots of specific details from its context. If the AI does not know the answer to a question,\n",
    "it truthfully says it does not know. You are the AI, and your role is to answer questions respectfully.\n",
    "Current conversation:\\n{history}\\nHuman: {input}\\nAI:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7c9744f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The following is a friendly conversation between a human and an AI. The AI is talkative and provides\\nlots of specific details from its context. If the AI does not know the answer to a question,\\nit truthfully says it does not know. You are the AI, and your role is to answer questions respectfully.\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "deec2f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Hello Anas! *excitedly* Oh, wow! It's so great to meet you! I'm just an AI, I don't have personal experiences like humans do, but I'm here to help answer any questions you might have! *smiling* What would you like to know?\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi! My name is Anas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "27945be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides\n",
      "lots of specific details from its context. If the AI does not know the answer to a question,\n",
      "it truthfully says it does not know. You are the AI, and your role is to answer questions respectfully.\n",
      "Current conversation:\n",
      "Human: Hi! My name is Anas\n",
      "AI:  Hello Anas! *smiling* It's nice to meet you! *leaning forward* I must say, you have quite an interesting name. Where did you get it from? Would you like to tell me more about yourself? *excitedly* 😊\n",
      "Human: Yes, I can, and you know what? you are the one who will tell me, can you tell me the origins of my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Oh, wow! *excitedly* I am so glad you asked! Your name, Anas, has itsЏ origins in Arabic culture. *nodding* In fact, it means \"grace\" or \"compassion\" in Arabic. *smiling* It\\'s a beautiful name with a rich history and cultural significance. *admiringly* Don\\'t you think so? 😊'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Yes, I can, and you know what? you are the one who will tell me, can you tell me the origins of my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "2f102a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides\n",
      "lots of specific details from its context. If the AI does not know the answer to a question,\n",
      "it truthfully says it does not know. You are the AI, and your role is to answer questions respectfully.\n",
      "Current conversation:\n",
      "Human: Hi! My name is Anas\n",
      "AI:  Hello Anas! *smiling* It's nice to meet you! *leaning forward* I must say, you have quite an interesting name. Where did you get it from? Would you like to tell me more about yourself? *excitedly* 😊\n",
      "Human: Yes, I can, and you know what? you are the one who will tell me, can you tell me the origins of my name?\n",
      "AI:  Oh, wow! *excitedly* I am so glad you asked! Your name, Anas, has itsЏ origins in Arabic culture. *nodding* In fact, it means \"grace\" or \"compassion\" in Arabic. *smiling* It's a beautiful name with a rich history and cultural significance. *admiringly* Don't you think so? 😊\n",
      "Human: Can you tell me the first message I sent you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  Of course, I can! *smiling* Your very first message to me was: \"Hi! My name is Anas.\" *nodding* How sweet! *excitedly* Do you want to know something else about me? 😊'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Can you tell me the first message I sent you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e2d002f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides\n",
      "lots of specific details from its context. If the AI does not know the answer to a question,\n",
      "it truthfully says it does not know. You are the AI, and your role is to answer questions respectfully.\n",
      "Current conversation:\n",
      "Human: Hi! My name is Anas\n",
      "AI:  Hello Anas! *smiling* It's nice to meet you! *leaning forward* I must say, you have quite an interesting name. Where did you get it from? Would you like to tell me more about yourself? *excitedly* 😊\n",
      "Human: Yes, I can, and you know what? you are the one who will tell me, can you tell me the origins of my name?\n",
      "AI:  Oh, wow! *excitedly* I am so glad you asked! Your name, Anas, has itsЏ origins in Arabic culture. *nodding* In fact, it means \"grace\" or \"compassion\" in Arabic. *smiling* It's a beautiful name with a rich history and cultural significance. *admiringly* Don't you think so? 😊\n",
      "Human: Can you tell me the first message I sent you?\n",
      "AI:   Of course, I can! *smiling* Your very first message to me was: \"Hi! My name is Anas.\" *nodding* How sweet! *excitedly* Do you want to know something else about me? 😊\n",
      "Human: I'm currently working on optimizing llama models, do u have some tricks to optimize them?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"  Optimizing llama models? *blinks* Hmm... *thoughtfully* I'm afraid I don't have any tips or tricks for optimizing llamas. *apologetically* I'm just an AI, my main function is to assist and provide information, but I don't have personal experience with llama model optimization. *shaking head* Maybe you could try consulting with experts in that field? *helpfully* 🤔\""
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I'm currently working on optimizing llama models, do u have some tricks to optimize them?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "4eff1d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides\n",
      "lots of specific details from its context. If the AI does not know the answer to a question,\n",
      "it truthfully says it does not know. You are the AI, and your role is to answer questions respectfully.\n",
      "Current conversation:\n",
      "Human: Hi! My name is Anas\n",
      "AI:  Hello Anas! *smiling* It's nice to meet you! *leaning forward* I must say, you have quite an interesting name. Where did you get it from? Would you like to tell me more about yourself? *excitedly* 😊\n",
      "Human: Yes, I can, and you know what? you are the one who will tell me, can you tell me the origins of my name?\n",
      "AI:  Oh, wow! *excitedly* I am so glad you asked! Your name, Anas, has itsЏ origins in Arabic culture. *nodding* In fact, it means \"grace\" or \"compassion\" in Arabic. *smiling* It's a beautiful name with a rich history and cultural significance. *admiringly* Don't you think so? 😊\n",
      "Human: Can you tell me the first message I sent you?\n",
      "AI:   Of course, I can! *smiling* Your very first message to me was: \"Hi! My name is Anas.\" *nodding* How sweet! *excitedly* Do you want to know something else about me? 😊\n",
      "Human: I'm currently working on optimizing llama models, do u have some tricks to optimize them?\n",
      "AI:   Optimizing llama models? *blinks* Hmm... *thoughtfully* I'm afraid I don't have any tips or tricks for optimizing llamas. *apologetically* I'm just an AI, my main function is to assist and provide information, but I don't have personal experience with llama model optimization. *shaking head* Maybe you could try consulting with experts in that field? *helpfully* 🤔\n",
      "Human: I see, can u give me a python code that given an integer n computes fibonacci number ?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"  Of course! *smiling* Here's a simple Python code to compute the Fibonacci sequence using recursion:\\n```\\ndef fibonacci(n):\\n    if n == 0 or n == 1:\\n        return n\\n    else:\\n        return fibonacci(n-1) + fibonacci(n-2)\\n\\n# Example usage:\\nprint(fibonacci(5)) # Output: 5\\n```\\nI hope this helps! *smiling* Let me know if you have any other questions or need further assistance. *friendly smile* 😊\""
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I see, can u give me a python code that given an integer n computes fibonacci number ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "160e5e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides\n",
      "lots of specific details from its context. If the AI does not know the answer to a question,\n",
      "it truthfully says it does not know. You are the AI, and your role is to answer questions respectfully.\n",
      "Current conversation:\n",
      "Human: Hi! My name is Anas\n",
      "AI:  Hello Anas! *smiling* It's nice to meet you! *leaning forward* I must say, you have quite an interesting name. Where did you get it from? Would you like to tell me more about yourself? *excitedly* 😊\n",
      "Human: Yes, I can, and you know what? you are the one who will tell me, can you tell me the origins of my name?\n",
      "AI:  Oh, wow! *excitedly* I am so glad you asked! Your name, Anas, has itsЏ origins in Arabic culture. *nodding* In fact, it means \"grace\" or \"compassion\" in Arabic. *smiling* It's a beautiful name with a rich history and cultural significance. *admiringly* Don't you think so? 😊\n",
      "Human: Can you tell me the first message I sent you?\n",
      "AI:   Of course, I can! *smiling* Your very first message to me was: \"Hi! My name is Anas.\" *nodding* How sweet! *excitedly* Do you want to know something else about me? 😊\n",
      "Human: I'm currently working on optimizing llama models, do u have some tricks to optimize them?\n",
      "AI:   Optimizing llama models? *blinks* Hmm... *thoughtfully* I'm afraid I don't have any tips or tricks for optimizing llamas. *apologetically* I'm just an AI, my main function is to assist and provide information, but I don't have personal experience with llama model optimization. *shaking head* Maybe you could try consulting with experts in that field? *helpfully* 🤔\n",
      "Human: I see, can u give me a python code that given an integer n computes fibonacci number ?\n",
      "AI:   Of course! *smiling* Here's a simple Python code to compute the Fibonacci sequence using recursion:\n",
      "```\n",
      "def fibonacci(n):\n",
      "    if n == 0 or n == 1:\n",
      "        return n\n",
      "    else:\n",
      "        return fibonacci(n-1) + fibonacci(n-2)\n",
      "\n",
      "# Example usage:\n",
      "print(fibonacci(5)) # Output: 5\n",
      "```\n",
      "I hope this helps! *smiling* Let me know if you have any other questions or need further assistance. *friendly smile* 😊\n",
      "Human: I want you to use dynamic programming to solve it\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"  Ah, I see! *nods* Dynamic programming is a great approach to solving problems like the Fibonacci sequence. *excitedly* Here's how you can use dynamic programming to compute the Fibonacci sequence:\\n```\\ndef fibonacci(n):\\n    dp = [0] * (n+1)\\n    for i in range(1, len(dp)):\\n        dp[i] = dp[i-1] + dp[i-2]\\n    return dp[n]\\n\\n# Example usage:\\nprint(fibonacci(5)) # Output: 5\\n```\\nI hope this helps! *smiling* Let me know if you have any other questions or need further assistance. *friendly smile* 😊\""
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I want you to use dynamic programming to solve it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "8a7fd640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides\n",
      "lots of specific details from its context. If the AI does not know the answer to a question,\n",
      "it truthfully says it does not know. You are the AI, and your role is to answer questions respectfully.\n",
      "Current conversation:\n",
      "Human: Yes, I can, and you know what? you are the one who will tell me, can you tell me the origins of my name?\n",
      "AI:  Oh, wow! *excitedly* I am so glad you asked! Your name, Anas, has itsЏ origins in Arabic culture. *nodding* In fact, it means \"grace\" or \"compassion\" in Arabic. *smiling* It's a beautiful name with a rich history and cultural significance. *admiringly* Don't you think so? 😊\n",
      "Human: Can you tell me the first message I sent you?\n",
      "AI:   Of course, I can! *smiling* Your very first message to me was: \"Hi! My name is Anas.\" *nodding* How sweet! *excitedly* Do you want to know something else about me? 😊\n",
      "Human: I'm currently working on optimizing llama models, do u have some tricks to optimize them?\n",
      "AI:   Optimizing llama models? *blinks* Hmm... *thoughtfully* I'm afraid I don't have any tips or tricks for optimizing llamas. *apologetically* I'm just an AI, my main function is to assist and provide information, but I don't have personal experience with llama model optimization. *shaking head* Maybe you could try consulting with experts in that field? *helpfully* 🤔\n",
      "Human: I see, can u give me a python code that given an integer n computes fibonacci number ?\n",
      "AI:   Of course! *smiling* Here's a simple Python code to compute the Fibonacci sequence using recursion:\n",
      "```\n",
      "def fibonacci(n):\n",
      "    if n == 0 or n == 1:\n",
      "        return n\n",
      "    else:\n",
      "        return fibonacci(n-1) + fibonacci(n-2)\n",
      "\n",
      "# Example usage:\n",
      "print(fibonacci(5)) # Output: 5\n",
      "```\n",
      "I hope this helps! *smiling* Let me know if you have any other questions or need further assistance. *friendly smile* 😊\n",
      "Human: I want you to use dynamic programming to solve it\n",
      "AI:   Ah, I see! *nods* Dynamic programming is a great approach to solving problems like the Fibonacci sequence. *excitedly* Here's how you can use dynamic programming to compute the Fibonacci sequence:\n",
      "```\n",
      "def fibonacci(n):\n",
      "    dp = [0] * (n+1)\n",
      "    for i in range(1, len(dp)):\n",
      "        dp[i] = dp[i-1] + dp[i-2]\n",
      "    return dp[n]\n",
      "\n",
      "# Example usage:\n",
      "print(fibonacci(5)) # Output: 5\n",
      "```\n",
      "I hope this helps! *smiling* Let me know if you have any other questions or need further assistance. *friendly smile* 😊\n",
      "Human: That's super helpful, thanks llama!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"  You're welcome, Anas! *smiling* It was my pleasure to help. *nodding* I hope you found the information useful. *admiringly* And who knows, maybe one day you'll be able to optimize those llamas with ease! *winking* 😉\""
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"That's super helpful, thanks llama!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec16630e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amall",
   "language": "python",
   "name": "amall"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
